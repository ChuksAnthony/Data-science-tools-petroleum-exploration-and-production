{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python data science tools for petroleum exploration and production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook goes with the blog post: [Visual data exploration in Python – correlation, confidence, spuriousness](https://mycarta.wordpress.com/2019/03/17/visual-data-exploration-in-python-correlation-confidence-spuriousness/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - data visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import the libraries we will need, and the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data loading and inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data (public) is from the paper **Many correlation coefficients, null hypoteses, and high value (Hunt, 2013, Reference 1)**; I used it previously in  my [Geoscience ML notebook 2](https://github.com/mycarta/predict/blob/master/Geoscience_ML_notebook_2.ipynb)\n",
    "The dependent variable to be predicted, Y, is oil production (measured in tens of barrels of oil per day) from a marine barrier sand.\n",
    "Independent variables are:\n",
    "\n",
    "- Gross pay in meters\n",
    "\n",
    "- Phi-h, with a 3% porosity cut-off\n",
    "\n",
    "- Position in the reservoir. This is a ranked variable where a value close to 1 means the upper reservoir facies, a value close to 2 means the middle reservoir facies, and a value close to 3 means the lower reservoir facies.\n",
    "\n",
    "- Pressure drawdown in MPa.\n",
    "\n",
    "- 2 random variables\n",
    "\n",
    "- 1 variable added by me (see my [Geoscience ML support notebook](https://github.com/mycarta/predict/blob/master/Geoscience_ML_support_for_notebook_2.ipynb)) as a log transform of Gross pay to simulate extremely highly correlated independent variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/Table2_Hunt_2013_edit.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gross pay</th>\n",
       "      <th>Phi-h</th>\n",
       "      <th>Position</th>\n",
       "      <th>Pressure</th>\n",
       "      <th>Random 1</th>\n",
       "      <th>Random 2</th>\n",
       "      <th>Gross pay transform</th>\n",
       "      <th>Production</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.1</td>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "      <td>379</td>\n",
       "      <td>3.54</td>\n",
       "      <td>15.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "      <td>269</td>\n",
       "      <td>5.79</td>\n",
       "      <td>21.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.9</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>245</td>\n",
       "      <td>8.51</td>\n",
       "      <td>22.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.1</td>\n",
       "      <td>21.7</td>\n",
       "      <td>2.1</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>273</td>\n",
       "      <td>11.52</td>\n",
       "      <td>15.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.1</td>\n",
       "      <td>24.6</td>\n",
       "      <td>2.9</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>237</td>\n",
       "      <td>10.16</td>\n",
       "      <td>7.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.9</td>\n",
       "      <td>39.2</td>\n",
       "      <td>1.1</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>278</td>\n",
       "      <td>11.14</td>\n",
       "      <td>22.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5.9</td>\n",
       "      <td>23.6</td>\n",
       "      <td>2.1</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>241</td>\n",
       "      <td>15.04</td>\n",
       "      <td>18.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13</td>\n",
       "      <td>20</td>\n",
       "      <td>269</td>\n",
       "      <td>15.10</td>\n",
       "      <td>24.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8.1</td>\n",
       "      <td>72.9</td>\n",
       "      <td>2.9</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>248</td>\n",
       "      <td>14.49</td>\n",
       "      <td>24.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8.9</td>\n",
       "      <td>35.6</td>\n",
       "      <td>2.8</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>210</td>\n",
       "      <td>16.90</td>\n",
       "      <td>25.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>16</td>\n",
       "      <td>21</td>\n",
       "      <td>334</td>\n",
       "      <td>16.61</td>\n",
       "      <td>36.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11.1</td>\n",
       "      <td>77.7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>340</td>\n",
       "      <td>17.81</td>\n",
       "      <td>36.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>11.9</td>\n",
       "      <td>71.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>20</td>\n",
       "      <td>11</td>\n",
       "      <td>224</td>\n",
       "      <td>19.74</td>\n",
       "      <td>39.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>16</td>\n",
       "      <td>9</td>\n",
       "      <td>338</td>\n",
       "      <td>17.70</td>\n",
       "      <td>51.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14.1</td>\n",
       "      <td>141.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>367</td>\n",
       "      <td>19.16</td>\n",
       "      <td>48.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15.1</td>\n",
       "      <td>105.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>363</td>\n",
       "      <td>21.97</td>\n",
       "      <td>51.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>15.9</td>\n",
       "      <td>79.5</td>\n",
       "      <td>1.1</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>395</td>\n",
       "      <td>22.15</td>\n",
       "      <td>59.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>16.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>17</td>\n",
       "      <td>15</td>\n",
       "      <td>295</td>\n",
       "      <td>24.24</td>\n",
       "      <td>58.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>17.1</td>\n",
       "      <td>85.5</td>\n",
       "      <td>1.9</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>266</td>\n",
       "      <td>23.58</td>\n",
       "      <td>41.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>18.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>2.8</td>\n",
       "      <td>18</td>\n",
       "      <td>19</td>\n",
       "      <td>210</td>\n",
       "      <td>23.77</td>\n",
       "      <td>44.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>19.1</td>\n",
       "      <td>114.6</td>\n",
       "      <td>2.1</td>\n",
       "      <td>10</td>\n",
       "      <td>21</td>\n",
       "      <td>366</td>\n",
       "      <td>29.25</td>\n",
       "      <td>37.51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Gross pay  Phi-h  Position  Pressure  Random 1  Random 2  \\\n",
       "0         0.1    0.5       2.1        19         5       379   \n",
       "1         1.0    4.0       1.1        16        13       269   \n",
       "2         1.9   19.0       1.0        14        12       245   \n",
       "3         3.1   21.7       2.1        17         6       273   \n",
       "4         4.1   24.6       2.9        11        10       237   \n",
       "5         4.9   39.2       1.1        12         7       278   \n",
       "6         5.9   23.6       2.1        13        13       241   \n",
       "7         7.0   63.0       2.0        13        20       269   \n",
       "8         8.1   72.9       2.9        14         1       248   \n",
       "9         8.9   35.6       2.8        16         1       210   \n",
       "10       10.0  100.0       2.2        16        21       334   \n",
       "11       11.1   77.7       2.0        14         1       340   \n",
       "12       11.9   71.4       2.9        20        11       224   \n",
       "13       13.0  117.0       1.1        16         9       338   \n",
       "14       14.1  141.0       1.2        14        10       367   \n",
       "15       15.1  105.7       1.0        17         3       363   \n",
       "16       15.9   79.5       1.1        20        10       395   \n",
       "17       16.0  160.0       1.2        17        15       295   \n",
       "18       17.1   85.5       1.9        14         6       266   \n",
       "19       18.0   90.0       2.8        18        19       210   \n",
       "20       19.1  114.6       2.1        10        21       366   \n",
       "\n",
       "    Gross pay transform  Production  \n",
       "0                  3.54       15.10  \n",
       "1                  5.79       21.30  \n",
       "2                  8.51       22.75  \n",
       "3                 11.52       15.72  \n",
       "4                 10.16        7.71  \n",
       "5                 11.14       22.67  \n",
       "6                 15.04       18.11  \n",
       "7                 15.10       24.30  \n",
       "8                 14.49       24.04  \n",
       "9                 16.90       25.11  \n",
       "10                16.61       36.80  \n",
       "11                17.81       36.42  \n",
       "12                19.74       39.59  \n",
       "13                17.70       51.60  \n",
       "14                19.16       48.05  \n",
       "15                21.97       51.72  \n",
       "16                22.15       59.20  \n",
       "17                24.24       58.30  \n",
       "18                23.58       41.80  \n",
       "19                23.77       44.20  \n",
       "20                29.25       37.51  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print a list of the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Gross pay', 'Phi-h', 'Position', 'Pressure', 'Random 1', 'Random 2', 'Gross pay transform', 'Production']\n"
     ]
    }
   ],
   "source": [
    "print (list(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can rearrange variable columns, then print a summary of the data and compare the row count to show there are no missing data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Position</th>\n",
       "      <th>Gross pay</th>\n",
       "      <th>Phi-h</th>\n",
       "      <th>Pressure</th>\n",
       "      <th>Random 1</th>\n",
       "      <th>Random 2</th>\n",
       "      <th>Gross pay transform</th>\n",
       "      <th>Production</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>21.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>21.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.885714</td>\n",
       "      <td>9.823810</td>\n",
       "      <td>68.880952</td>\n",
       "      <td>15.285714</td>\n",
       "      <td>10.190476</td>\n",
       "      <td>292.714286</td>\n",
       "      <td>16.579524</td>\n",
       "      <td>33.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.708721</td>\n",
       "      <td>5.948521</td>\n",
       "      <td>45.167894</td>\n",
       "      <td>2.759400</td>\n",
       "      <td>6.439092</td>\n",
       "      <td>59.429069</td>\n",
       "      <td>6.543793</td>\n",
       "      <td>15.141909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>210.000000</td>\n",
       "      <td>3.540000</td>\n",
       "      <td>7.710000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.100000</td>\n",
       "      <td>4.900000</td>\n",
       "      <td>24.600000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>245.000000</td>\n",
       "      <td>11.520000</td>\n",
       "      <td>22.670000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>72.900000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>273.000000</td>\n",
       "      <td>16.900000</td>\n",
       "      <td>36.420000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.200000</td>\n",
       "      <td>15.100000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>340.000000</td>\n",
       "      <td>21.970000</td>\n",
       "      <td>44.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.900000</td>\n",
       "      <td>19.100000</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>29.250000</td>\n",
       "      <td>59.200000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Position  Gross pay       Phi-h   Pressure   Random 1    Random 2  \\\n",
       "count  21.000000  21.000000   21.000000  21.000000  21.000000   21.000000   \n",
       "mean    1.885714   9.823810   68.880952  15.285714  10.190476  292.714286   \n",
       "std     0.708721   5.948521   45.167894   2.759400   6.439092   59.429069   \n",
       "min     1.000000   0.100000    0.500000  10.000000   1.000000  210.000000   \n",
       "25%     1.100000   4.900000   24.600000  14.000000   6.000000  245.000000   \n",
       "50%     2.000000  10.000000   72.900000  16.000000  10.000000  273.000000   \n",
       "75%     2.200000  15.100000  100.000000  17.000000  13.000000  340.000000   \n",
       "max     2.900000  19.100000  160.000000  20.000000  21.000000  395.000000   \n",
       "\n",
       "       Gross pay transform  Production  \n",
       "count            21.000000   21.000000  \n",
       "mean             16.579524   33.428571  \n",
       "std               6.543793   15.141909  \n",
       "min               3.540000    7.710000  \n",
       "25%              11.520000   22.670000  \n",
       "50%              16.900000   36.420000  \n",
       "75%              21.970000   44.200000  \n",
       "max              29.250000   59.200000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.loc[:, ['Position', 'Gross pay', 'Phi-h', 'Pressure', 'Random 1', 'Random 2', 'Gross pay transform', 'Production']]\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also explicitly test to ensure data does not have no missing measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization with a custom correlation matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It includes bivariate scatter-plots in the upper triangle, contours in the lower triangle, shape of the bivariate distributions on the diagonal.\n",
    "\n",
    "To ignore future warning for a [known issue](https://github.com/h5py/h5py/issues/974):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding a utility function to annotate individual scatterplots with rank correlation coefficient. Adapted from [this Stack Overflow answer](https://stackoverflow.com/a/30942817)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def corrfunc(x, y, **kws):\n",
    "    r, _ = sp.stats.spearmanr(x, y)\n",
    "    ax = plt.gca()\n",
    "    ax.annotate(\"CC = {:.2f}\".format(r), xy=(.1, .97), xycoords=ax.transAxes, color = 'g', fontsize = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"axes.labelsize\"] = 18\n",
    "g = sns.PairGrid(data, diag_sharey=False)\n",
    "axes = g.axes\n",
    "\n",
    "g.map_upper(plt.scatter,  linewidths=1, edgecolor=\"w\", s=90, alpha = 0.5)\n",
    "#g.map_upper(corrfunc)\n",
    "\n",
    "g.map_diag(sns.kdeplot, lw = 4, legend=False)\n",
    "g.map_lower(sns.kdeplot, cmap=\"Blues_d\")\n",
    "\n",
    "#plt.savefig('Matrix_initial.png', dpi=400, bbox_inches='tight', pad_inches=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preliminary observations:\n",
    "-  Position (ranked position within the reservoir) seems to be a very important variable nothwidstanding the significance tests. Indeed, Gross-pay, Phi-h, Pressure, and Production all cluster according to position in three separate groups.\n",
    "- Linear relationships with Production: Gross pay and Phi-h and Production, both with high correlation coefficients; Pressure, with more scatter and lower correlation coefficient, perhaps Random 2. \n",
    "- Linear relationships amongst independent variables: Gross with Phi-h, and Gross pay with its transform.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import rc\n",
    "font = {'size'   : 18}\n",
    "rc('font', **font)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - when is a correlation significant?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a - Kalkomey's probability of spuriousness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of a single (seismic) attribute, for example P-impedance, as a possible predictor of a reservoir porperty, for example porosity,  **Kalkomey (1997, Reference 3)** defined spuriousness as the probability of observing the absolute value of the sample correlation, r, being greater than some constant, R, given the true (population) correlation ρ is zero is given by:\n",
    "\n",
    "\\begin{equation*}\n",
    "p_{sc} = Pr (|r|>= R)= Pr \\Bigl( |t| >= \\frac{R \\sqrt{n-2}}{\\sqrt{1-R^2}} \\Bigr)\n",
    "\\end{equation*}\n",
    "\n",
    "where n is the sample size or number of locations (the wells) with measurements of both the reservoir property and the seismic attribute, and t is the Student’s t distribution with n-2 degrees of freedom.\n",
    "\n",
    "This probability of spuriousness psc is the same as the well-known p-value, and we can rephrase the above as per my definition of p-value: p it is the probability of getting a sample with at least the correlation coefficient we just got or even higher, purely by chance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With multiple independent attributes, the probability of observing at least one spurious correlation when considering a set of k independent attributes is simply 1 minus the probability that none of the sample correlations are spurious, which is equivalent to the summation:\n",
    "\n",
    "\\begin{equation*}\n",
    "1-(1-p_{sc})^{k} =  \\sum_{i=1}^k  p_{sc}(1-p_{sc})^{(i-1)}\n",
    "\\end{equation*}\n",
    "\n",
    "where \n",
    "\n",
    "\\begin{equation*}\n",
    "p_{sc}(1-p_{sc})^{(i-1)}\n",
    "\\end{equation*}\n",
    "\n",
    "is the penalty for increasing the number of attributes considered from k-1 to k."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The function below calculates psc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is an implementation of the summation above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def P_spurious (r, nwells, nattributes):\n",
    "    t_of_r = r * np.sqrt((nwells-2)/(1-np.power(r,2)))  \n",
    "    p = sp.stats.t.sf(np.abs(t_of_r), nwells-2)*2 \n",
    "    ks = np.arange(1, nattributes+1, 1)\n",
    "    return np.sum(p * np.power(1-p, ks-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the probability of spuriousness depends solely on the number of independent attributes, the number of wells n and the magnitude R of the spurious sample correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replicating Table 3 from Kalkomey's paper: psc with 10 attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a range for the number of wells and for the constant R, the magnitude of the spurious sample correlation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nrs = np.arange(0.1, 1, 0.1).round(decimals=2) # to avoid odd tick label problem\n",
    "wells= np.asarray([5, 10, 15, 20, 25, 35, 50, 75, 100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the number of attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ps = np.asarray([P_spurious(R, nw, na)  for R in nrs for nw in wells]).reshape(9,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "psdf = pd.DataFrame(ps, index = [\" R=\" + mp for mp in map(str, nrs)], \n",
    "                  columns = [\" n=\" + mp for mp in map(str, wells)]).round(decimals=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "ax = sns.heatmap(psdf, annot=True, annot_kws={\"size\": 16}, cmap='Oranges', square=True, linewidths=0.01, cbar=False)\n",
    "plt.yticks(rotation=0) \n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Probability of spurious correlation, 10 attributes')\n",
    "ax.set_ylim(psdf.shape[0]-0.01, -0.01)\n",
    "#plt.savefig('Kalcomey_T3.png', dpi=400, bbox_inches='tight', pad_inches=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Replicating Table 1 from Kalkomey's paper: psc with 1 attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When there is only one attribute the probability of spuriousness depends only on the number of wells n and the magnitude R of the spurious sample correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ps = np.asarray([P_spurious(R, nw, na)  for R in nrs for nw in wells]).reshape(9,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "psdf = pd.DataFrame(ps, index = [\" R=\" + mp for mp in map(str, nrs)], \n",
    "                  columns = [\" n=\" + mp for mp in map(str, wells)]).round(decimals=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "ax = sns.heatmap(psdf, annot=True,annot_kws={\"size\": 16}, cmap='Oranges', square=True, linewidths=0.01, cbar=False)\n",
    "plt.yticks(rotation=0) \n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Probability of spurious correlation, 1 attribute')\n",
    "ax.set_ylim(psdf.shape[0]-0.01, -0.01)\n",
    "#plt.savefig('Kalcomey_T1.png', dpi=400, bbox_inches='tight', pad_inches=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**So, for example, with 10 wells and a correlation coefficient CC=0.4 between impedance and porosity (so we're comparing against R = 0.4), theres a 25% probability, or 1 in 4 chances, that the correlation is spurious.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Showing the dependence of psc on the number of wells and the magnitude of the spurious sample correlation, for 1 attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do this by plotting together the left-most 4 columns of the last table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrs = np.arange(0, 1, 0.05).round(decimals=2)\n",
    "wells= np.arange(5, 23, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ps = np.asarray([P_spurious(R, nw, 1)  for R in nrs for nw in wells]).reshape(20,18)\n",
    "psdf = pd.DataFrame(ps, index = [\" R=\" + mp for mp in map(str, nrs)], \n",
    "                  columns = [\" n=\" + mp for mp in map(str, wells)]).round(decimals=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.subplots(figsize=(10, 10))\n",
    "plt.plot(nrs, psdf[' n=5'], 'r', label = '5 wells')\n",
    "plt.plot(nrs, psdf[' n=10'], 'Orange', label = '10 wells')\n",
    "plt.plot(nrs, psdf[' n=15'], 'limegreen', label = '15 wells')\n",
    "plt.plot(nrs, psdf[' n=20'], 'navy', label = '20 wells')\n",
    "plt.xticks(np.arange(0, 1, 0.05).round(decimals=2), rotation = 60)\n",
    "plt.yticks(np.arange(0, 1.05, 0.1))\n",
    "plt.ylabel('PS')\n",
    "plt.xlabel('R')\n",
    "plt.title('Probability of spurious correlation, dependence on wells')\n",
    "plt.grid()\n",
    "plt.legend();\n",
    "#plt.savefig('Kalcomey_PS_wells.png', dpi=400, bbox_inches='tight', pad_inches=0.1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With R = 0.85 - the sample correlation coefficient between Production and Gross pay -  and 20 wells, 15 wells, or even 10 wells the probability of spuriousness is 0; even with just 5 wells it would be only 6.8% or about 1 in 15.\n",
    "\n",
    "BUt with R = 0.38 - the sample correlation coefficient between Production and Pressure drawdown - the probability of spuriousness with 20 wells is 9%; with 15 wells it is 16%; with 10 wells it is 28% (already greater than 1 in 4 chances); with 5 wells it is 53%, or 1 chance in 2.\n",
    "\n",
    "This does not mean that we should not use Pressure to estimate Production. I certainly would have not a lot of confidence in using it as a sole predictor of Production  but because we are testing a hypothesis based on physical expectations (through Darcy's Law, as pointed out by Lee Hunt) I would use it in a multivariate estimation of Production. Things would be different had this been an unknown variable without a hypotesing explaining the relationship: this is where the domain knowledge comes into play.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Showing the dependence of psc on the number of attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps1 = np.asarray([P_spurious(R, nw, 1)  for R in nrs for nw in wells]).reshape(20,18)\n",
    "psdf1 = pd.DataFrame(ps1, index = [\" R=\" + mp for mp in map(str, nrs)], \n",
    "                  columns = [\" n=\" + mp for mp in map(str, wells)]).round(decimals=2)\n",
    "\n",
    "ps2 = np.asarray([P_spurious(R, nw, 2)  for R in nrs for nw in wells]).reshape(20,18)\n",
    "psdf2 = pd.DataFrame(ps2, index = [\" R=\" + mp for mp in map(str, nrs)], \n",
    "                  columns = [\" n=\" + mp for mp in map(str, wells)]).round(decimals=2)\n",
    "\n",
    "ps4 = np.asarray([P_spurious(R, nw, 4)  for R in nrs for nw in wells]).reshape(20,18)\n",
    "psdf4 = pd.DataFrame(ps4, index = [\" R=\" + mp for mp in map(str, nrs)], \n",
    "                  columns = [\" n=\" + mp for mp in map(str, wells)]).round(decimals=2)\n",
    "\n",
    "ps8 = np.asarray([P_spurious(R, nw, 8)  for R in nrs for nw in wells]).reshape(20,18)\n",
    "psdf8 = pd.DataFrame(ps8, index = [\" R=\" + mp for mp in map(str, nrs)], \n",
    "                  columns = [\" n=\" + mp for mp in map(str, wells)]).round(decimals=2)\n",
    "\n",
    "ps16 = np.asarray([P_spurious(R, nw, 16)  for R in nrs for nw in wells]).reshape(20,18)\n",
    "psdf16 = pd.DataFrame(ps16, index = [\" R=\" + mp for mp in map(str, nrs)], \n",
    "                  columns = [\" n=\" + mp for mp in map(str, wells)]).round(decimals=2)\n",
    "\n",
    "ps32 = np.asarray([P_spurious(R, nw, 32)  for R in nrs for nw in wells]).reshape(20,18)\n",
    "psdf32 = pd.DataFrame(ps32, index = [\" R=\" + mp for mp in map(str, nrs)], \n",
    "                  columns = [\" n=\" + mp for mp in map(str, wells)]).round(decimals=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.subplots(figsize=(10, 10))\n",
    "plt.plot(nrs, psdf1[' n=21'], 'k', label = '1 attribute')\n",
    "plt.plot(nrs, psdf2[' n=21'], 'navy', label = '2 attributes')\n",
    "plt.plot(nrs, psdf4[' n=21'], 'limegreen', label = '4 attributes')\n",
    "plt.plot(nrs, psdf8[' n=21'], 'Orange', label = '8 attributes')\n",
    "plt.plot(nrs, psdf16[' n=21'], 'r', label = '16 attributes')\n",
    "plt.plot(nrs, psdf32[' n=21'], 'm', label = '32 attributes')\n",
    "plt.xticks(np.arange(0, 1, 0.05).round(decimals=2), rotation = 60)\n",
    "plt.yticks(np.arange(0, 1.05, 0.1))\n",
    "plt.ylabel('PS')\n",
    "plt.xlabel('R')\n",
    "plt.title('Probability of spurious correlation, dependence on attributes')\n",
    "plt.grid()\n",
    "plt.legend();\n",
    "#plt.savefig('Kalcomey_PS_attributes.png', dpi=400, bbox_inches='tight', pad_inches=0.1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, suppose we had 21 wells and used 2 attributes to predict Production, there would be a probability of 8% of observing a sample correlation coefficient of 0.45 for at least one of the attributes, when no correlation actually existed between the attributes and Production.\n",
    "The probability would go up to 15% for at least 1 out of 4 attributes, 28% for at least 1 out of 8, and so on.. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b - Critical r test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This test will give the value of correlation coefficient (critical correlation coefficient) above which one can rule out chance as an explanation for the relationship in the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def r_crit(nwells, a):\n",
    "    # a is equal to alpha/2, where alpha is the level of significance,\n",
    "    # the complement of the confidence level\n",
    "    t = sp.stats.t.isf(a, nwells-2) # nwells-2 is the degrees of freedom\n",
    "    r_crit = t/np.sqrt((nwells-2)+ np.power(t,2))\n",
    "    return r_crit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a useful test in its own right. For example, with our 21 wells, and confidence level of 95% (alpha/2 = 0.025), we see that the critical r is 0.433:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (r_crit(21, 0.025))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this test alone, one can argue that the correlation coefficients between Production and both Gross pay and Phi-h are significant (that is not due to chance); we cannot reach the same conclusion for Position, Pressure, Random 1, and Random 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, I find it more interesting to look at it in combination with the confidence interval for the population correlation coefficient, as shown in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c - Confidence interval for the population correlation coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another question we might ask, with a given number of wells and the sample correlation coefficient, is: \"_what is the 95% confidence interval for the correlation coefficient in the population?_\"\n",
    "\n",
    "And here's a convenient function to calculate that confidence interval, which I implemented following [Brown](http://brownmath.com/stat/correl.htm#CI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confInt(r, nwells):\n",
    "    z_crit = sp.stats.norm.ppf(.975) # critical z (confidence  \n",
    "                                     # interval bounded by +/- z)\n",
    "    std_Z = 1/np.sqrt(nwells-3)      # std dev of Fisher Z \n",
    "    E = z_crit*std_Z                 # error of the estimate\n",
    "    Z_star = 0.5*(np.log((1+r)/(1.0000000000001-r))) #Fisher Z\n",
    "    ZCI_l = Z_star - E\n",
    "    ZCI_u = Z_star + E               # confidence interval for Z\n",
    "    RCI_l = (np.exp(2*ZCI_l)-1)/(np.exp(2*ZCI_l)+1) \n",
    "    RCI_u = (np.exp(2*ZCI_u)-1)/(np.exp(2*ZCI_u)+1)\n",
    "    return RCI_u, RCI_l              # # confidence interval for ρ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick test to ensure we are getting same result as [Brown's example](http://brownmath.com/stat/correl.htm#CI):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (round(confInt(0.84, 25)[0], 3), round(confInt(0.84, 25)[1], 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way I use this function is to create a set of confidence intervals for the population correlation coefficient as a function of the observed sample correlation coefficient from 0 to 1, in increments of 0.5, and turn the result in a heatmap.\n",
    "\n",
    "Let's create the heatmap, then discuss the insights.\n",
    "I will use 21 wells as per our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crr = np.arange(0.0,1.05,0.05).round(decimals=2)\n",
    "cm21 = np.asarray(confInt(crr, 21)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice above that the function takes directly arrays as parameters, thanks to NumPy broadcasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CI21 = pd.DataFrame(cm21, index = ['CI_UPPER', 'CI_LOWER'], \n",
    "                    columns = [\" r = \" + mp for mp in map(str, crr)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.7)\n",
    "fig, ax = plt.subplots(figsize=(22, 4))\n",
    "h = sns.heatmap(CI21, annot=True, cmap='PuOr', square=True, linewidths=0.01, cbar=False)\n",
    "plt.xticks(rotation=45)\n",
    "h.add_patch(Rectangle((8,0), 1, 1, fill=True, color = 'magenta')) \n",
    "h.add_patch(Rectangle((8,1), 1, 1, fill=True, color = 'yellow')) \n",
    "h.add_patch(Rectangle((9,0), 1, 1, fill=True, color = 'limegreen')) \n",
    "h.add_patch(Rectangle((9,1), 1, 1, fill=True, color = 'green'))\n",
    "ax.set_ylim(CI21.shape[0]-0.01, -0.01);\n",
    "#plt.savefig('CI_heatmap_21.png', dpi=400, bbox_inches='tight', pad_inches=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us a bit more than just the critical r test. \n",
    "\n",
    "We see that for the correlation of ** Production and Gross Pay (sample cc = 0.85)**, the 95% confidence interval for the population correlation coefficient is 0.66-0.94 (not bad!). On the other hand, for the correlation of ** Production and Pressure (sample cc = 0.38)** the lower confidence limit is below zero, which means we cannot confidently reject the hypothesis that there is no linear association between Pressure and Production in the population. \n",
    "\n",
    "**N.B.** This DOES NOT imply that there isn’t a linear association between Pressure and Production, though; it just means that with the available data we can’t reach a conclusion either way, and more data points are needed. \n",
    "\n",
    "There's another interesting point that we can take home, follwing a similar discussion in  Chambers and Yarus (2002, Reference 4), which is that it that from this heatmap we can evince it would take at least a correlation coefficient of 0.45 (the critical r = 0.43, with a bit of rounding) for a correlation to have both upper and lower confidence limits positive; **BUT** even then, the lower confidence limit would still only be 0.023."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is interesting to see what happens with fewer wells, say 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm5 = np.asarray(confInt(crr, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CI5 = pd.DataFrame(cm5, index = ['CI_UPPER', 'CI_LOWER'], columns = [\" r = \" + mp for mp in map(str, crr)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.7)\n",
    "fig, ax = plt.subplots(figsize=(22, 4))\n",
    "h = sns.heatmap(CI5, annot=True, cmap='PuOr', square=True, linewidths=0.01, cbar=False)\n",
    "plt.xticks(rotation=45)\n",
    "h.add_patch(Rectangle((17,0), 1, 1, fill=True, color = 'magenta')) \n",
    "h.add_patch(Rectangle((17,1), 1, 1, fill=True, color = 'yellow')) \n",
    "ax.set_ylim(CI5.shape[0]-0.01, -0.01);\n",
    "#plt.savefig('CI_heatmap_5.png', dpi=400, bbox_inches='tight', pad_inches=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NB: With only 5 wells even Gross Pay does not pass this test!!__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - revised data visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization with a more informative custom correlation matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, here’s my ideal first visualization of data:\n",
    "\n",
    "- the correlation coefficient is coloured green if it is larger than the critical r, else coloured in purple\n",
    "- the confidence interval is coloured green if both lower and upper are larger than the critical r, else coloured in purple\n",
    "- the probability of spurious corrrelation is coloured in green when below 0.05 (or 5% chance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_crit21 = r_crit(21, 0.025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrfunc(x, y, rc = r_crit21, **kws):\n",
    "    r, p = sp.stats.spearmanr(x, y)\n",
    "    u, l = confInt(r, 21)  \n",
    "    if r > rc:\n",
    "       rclr = 'g'\n",
    "    else:\n",
    "        rclr= 'm' \n",
    "    if p > 0.05:\n",
    "       pclr = 'm'\n",
    "    else:\n",
    "        pclr= 'g'\n",
    "    ax = plt.gca()\n",
    "    ax.annotate(\"CC = {:.2f}\".format(r), xy=(.1, 1.25), xycoords=ax.transAxes, color = rclr, fontsize = 14)\n",
    "    ax.annotate(\"CI = [{:.2f} {:.2f}]\".format(u, l), xy=(.1, 1.1), xycoords=ax.transAxes, color = rclr, fontsize = 14)\n",
    "    ax.annotate(\"PS = {:.3f}\".format(p), xy=(.1, .95), xycoords=ax.transAxes, color = pclr, fontsize = 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"axes.labelsize\"] = 18\n",
    "g = sns.PairGrid(data, diag_sharey=False)\n",
    "axes = g.axes\n",
    "\n",
    "g.map_upper(plt.scatter,  linewidths=1, edgecolor=\"w\", s=90, alpha = 0.5)\n",
    "g.map_upper(corrfunc)\n",
    "\n",
    "g.map_diag(sns.kdeplot, lw = 4, legend=False)\n",
    "g.map_lower(sns.kdeplot, cmap=\"Blues_d\")\n",
    "\n",
    "#plt.savefig('matrix_final.png', dpi=500, bbox_inches='tight', pad_inches=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I created this list of hypothesis testing definitions drawing primarily from Stan Brown's<a href=\"http://brownmath.com/swt/\"> Stats without tears</a>, and secondarily form the reference papers below.\n",
    "\n",
    "For a sample n of measurements (observations) of variable x and variable y, with a linear correlation coefficient, r = 0.5. ρ is the correlation in the population we are trying to make inferences on.\n",
    "\n",
    "- **null hypothesis**: ρ = 0, the correlation in the population is zero, there is no linear relationship between the two variables. Our sample with its correlation coefficient r was just the luck of the draw.\n",
    "- **critical r**: the value of the correlation coefficient at which you can rule out chance as an explanation for the relationship in the sample.\n",
    "- **significance level, alpha**: the chance of being wrong that one accepts to live with; 0.05, or 5%, is a typical choice.\n",
    "- **degrees of freedom, df**: the number of observations minus the number of parameters to be estimated. In regression, because one observation is spent to estimate the slope, and one to estimate the intercept, df = n-2.\n",
    "- **p-value**: the probability of getting a sample with at least the correlation coefficient one just got, or even higher, purely by chance. If the p-value is small, the sample is in conflict with the null hypothesis. If the p-value is large, one can reach no conclusions about the population, and cannot reject the null hypothesis. p-value is the same as the **probability of spurious correlation** defined in **Kalkomey**.\n",
    "- **type I error**: rejecting the null hypothesis when it is actually true, e.g. using the sample to infer a non existing relationship in the population. This is the most costly type of error, leading to an inaccurate prediction with confidence, as described in both **Kalkomey** and **Chambers and Yarus**.\n",
    "- **type II error**: this error conversely occurs when a real correlation exists in the population but we fail to use the predictor. Type II errors are not as bad as Type I errors because they result in more uncertainty than justified, as opposed to inaccurate prediction with confidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) <a href=\"http://csegrecorder.com/features/view/value-of-integrated-geophysics-201312\"> Many correlation coefficients, null hypoteses, and high value.</a> Lee Hunt, CSEG Recorder, December 2013.\n",
    "\n",
    "2) <a href=\"http://csegrecorder.com/articles/view/too-many-seismic-attributes\"> Too many seismic attributes?</a> Arthur Barnes, CSEG Recorder, March 2006.\n",
    "\n",
    "\n",
    "3) <a href=\"http://library.seg.org/doi/abs/10.1190/1.1437610\"> Potential risks when using seismic attributes as predictors of reservoir properties.</a> Cynthia Kalkomey, The Leading Edge, March 1997.\n",
    "\n",
    "4) <a href=\"http://csegrecorder.com/articles/view/too-many-seismic-attributes\"> Quantitative use of seismic attributes for reservoir characterization.</a>  Richard Chambers and Jeffrey Yarus, CSEG Recorder, June 2002."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
